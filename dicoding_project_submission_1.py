# -*- coding: utf-8 -*-
"""Dicoding Project Submission 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D-Cz5TrCd04oHJ_0BueVtKQel1Bzmmbr

# Asteroid Hazard Classification - Submission Machine Learning Terapan
Nama : Made Pranajaya Dibyacita

Dicoding Profile: https://www.dicoding.com/users/mdprana/academies

## Project Overview
Asteroid adalah benda langit berbatu yang mengorbit Matahari dan memiliki
ukuran yang lebih kecil daripada planet. Ribuan asteroid telah diidentifikasi dan dikatalogkan oleh para astronom. Beberapa di antaranya dikategorikan sebagai Potentially Hazardous Asteroids (PHAs) atau asteroid yang berpotensi berbahaya karena orbitnya yang dekat dengan orbit Bumi dan ukurannya yang cukup besar untuk menyebabkan kerusakan signifikan jika terjadi tumbukan.

Memprediksi apakah sebuah asteroid berpotensi berbahaya atau tidak adalah masalah penting dalam astronomi dan perlindungan planet. NASA dan badan antariksa lainnya secara aktif melacak dan mengklasifikasikan asteroid untuk mengidentifikasi potensi ancaman. Dengan memanfaatkan machine learning, kita dapat mengembangkan model yang memprediksi status bahaya asteroid berdasarkan karakteristik fisik dan orbitalnya.

Proyek ini bertujuan untuk mengembangkan model machine learning yang dapat mengklasifikasikan asteroid sebagai berbahaya atau tidak berbahaya berdasarkan parameter-parameter seperti diameter, parameter orbital (eksentrisitas, sumbu semi-mayor, jarak perihelion), dan parameter fisik lainnya.


**Referensi:**
- [NASA Center for Near Earth Object Studies](https://cneos.jpl.nasa.gov/)
- [Minor Planet Center](https://www.minorplanetcenter.net/)

## Business Understanding

### Problem Statements

1. Bagaimana cara mengidentifikasi asteroid yang berpotensi berbahaya bagi Bumi dengan memanfaatkan data parameter fisik dan orbital asteroid?
2. Fitur apa saja yang paling berpengaruh dalam menentukan apakah sebuah asteroid berpotensi berbahaya atau tidak?
3. Seberapa akurat model machine learning dapat memprediksi status bahaya sebuah asteroid?

### Goals

1. Mengembangkan model machine learning yang dapat mengklasifikasikan asteroid sebagai berbahaya atau tidak berbahaya dengan akurasi tinggi.
2. Mengidentifikasi fitur-fitur yang paling signifikan dalam menentukan status bahaya asteroid.
3. Memberikan alat prediksi yang dapat digunakan untuk mengevaluasi asteroid baru yang ditemukan.

### Solution approach

Untuk menyelesaikan masalah klasifikasi ini, saya akan menggunakan beberapa algoritma machine learning dan membandingkan performanya:

1. **Logistic Regression**: Model dasar yang mudah diinterpretasi untuk klasifikasi biner.
2. **Random Forest Classifier**: Algoritma ensemble yang dapat menangani data non-linear dan memberikan informasi tentang feature importance.
3. **XGBoost**: Algoritma gradient boosting yang sering memberikan performa tinggi untuk masalah klasifikasi.

Pendekatan ini akan memungkinkan kita untuk membandingkan berbagai model dan memilih yang terbaik berdasarkan metrik evaluasi seperti akurasi, presisi, recall, dan F1-score.

## Data Understanding

Data yang digunakan pada proyek machine learning ini adalah **Asteroid Dataset (2023)** yang didapat dari situs Kaggle. Dataset ini berisi informasi tentang asteroid yang telah diidentifikasi, termasuk parameter fisik dan orbital mereka.

Link dataset: [Asteroid Dataset](https://www.kaggle.com/datasets/sakhawat18/asteroid-dataset)

Pertama, mari kita instal dan import library yang dibutuhkan, lalu unduh dataset dari Kaggle.
"""

# Install kaggle
!pip install kaggle

# Upload kaggle.json jika belum ada
from google.colab import files
import os

# Cek apakah file kaggle.json sudah ada
if not os.path.exists('/root/.kaggle'):
    os.makedirs('/root/.kaggle')

# Jika kaggle.json belum ada, upload file
if not os.path.exists('/root/.kaggle/kaggle.json'):
    print("Please upload your kaggle.json file")
    uploaded = files.upload()

    # Move the uploaded file to the correct location
    with open('/root/.kaggle/kaggle.json', 'w') as f:
        f.write(open(list(uploaded.keys())[0]).read())

    # Set the correct permissions
    !chmod 600 /root/.kaggle/kaggle.json

# Download dataset
!kaggle datasets download -d sakhawat18/asteroid-dataset
!unzip asteroid-dataset.zip

"""### Load dan Eksplorasi Dataset

Mari kita load dataset asteroid dan melihat struktur datanya.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from collections import Counter
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.metrics import f1_score, precision_score, recall_score
from sklearn.model_selection import RandomizedSearchCV

# Load dataset
df = pd.read_csv('dataset.csv')

# Tampilkan beberapa baris pertama
print("Sampel data asteroid:")
df.head()

"""#### Informasi Dataset"""

# Melihat informasi dataset
print("\nInformasi Dataset:")
df.info()

# Statistik deskriptif
print("\nStatistik Deskriptif:")
df.describe()

"""#### Cek Missing Values"""

# Memeriksa missing values
print("\nJumlah missing values per kolom:")
df.isnull().sum()

"""#### Distribusi Target: Potentially Hazardous Asteroid (PHA)"""

# Melihat distribusi kelas target (pha - potentially hazardous asteroid)
print("\nDistribusi Hazardous vs Non-Hazardous Asteroids:")
pha_count = df['pha'].value_counts()
print(pha_count)

# Visualisasi distribusi target
plt.figure(figsize=(8, 6))
sns.countplot(x='pha', data=df)
plt.title('Distribusi Asteroid Berbahaya vs Tidak Berbahaya')
plt.xlabel('Berbahaya (Y: Ya, N: Tidak)')
plt.ylabel('Jumlah')
plt.show()

"""Dari visualisasi di atas, terlihat bahwa kelas target sangat tidak seimbang. Asteroid yang diklasifikasikan sebagai berbahaya (Y) jauh lebih sedikit dibandingkan dengan yang tidak berbahaya (N). Kita perlu menangani ketidakseimbangan kelas ini pada tahap preprocessing.

#### Eksplorasi Fitur Numerik
"""

# Memilih fitur numerik yang relevan untuk analisis
numeric_features = ['H', 'diameter', 'albedo', 'e', 'a', 'q', 'i', 'moid', 'moid_ld']

# Histogram untuk fitur numerik
plt.figure(figsize=(15, 15))
for i, feature in enumerate(numeric_features):
    plt.subplot(3, 3, i+1)
    sns.histplot(data=df, x=feature, hue='pha', kde=True, bins=30)
    plt.title(f'Distribusi {feature}')
plt.tight_layout()
plt.show()

"""Mari kita lihat deskripsi dari beberapa fitur penting:

- **H**: Magnitude absolut (makin kecil nilainya, makin besar asteroidnya)
- **diameter**: Diameter asteroid dalam kilometer
- **albedo**: Reflektivitas permukaan asteroid
- **e**: Eksentrisitas orbit (0 = melingkar, mendekati 1 = sangat lonjong)
- **a**: Sumbu semi-mayor orbit dalam satuan AU (Astronomical Unit)
- **q**: Jarak perihelion (titik terdekat dengan Matahari) dalam AU
- **i**: Inklinasi orbit dalam derajat
- **moid**: Minimum Orbit Intersection Distance dengan Bumi dalam AU
- **moid_ld**: MOID dalam satuan jarak lunar (LD) dari Bumi

#### Korelasi antar Fitur
"""

# Mengisi missing values dengan median terlebih dahulu untuk perhitungan korelasi
df_numeric = df[numeric_features + ['pha']].copy()

# Konversi pha dari Y/N ke numerik (1/0) terlebih dahulu
df_numeric['pha'] = df_numeric['pha'].map({'Y': 1, 'N': 0})

# Sekarang isi nilai yang hilang pada kolom numerik lainnya
for col in numeric_features:  # Hanya isi kolom numerik, bukan 'pha'
    if df_numeric[col].isnull().sum() > 0:
        df_numeric[col] = df_numeric[col].fillna(df_numeric[col].median())

# Hitung korelasi antar fitur
correlation = df_numeric.corr()

# Visualisasi korelasi dengan heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Korelasi antar Fitur')
plt.show()

"""#### Korelasi Fitur dengan Target"""

# Korelasi antara fitur dan target (pha)
correlation_with_target = correlation['pha'].sort_values(ascending=False)
print("\nKorelasi Fitur dengan Target (PHA):")
print(correlation_with_target)

# Visualisasi top fitur berdasarkan korelasi dengan target
plt.figure(figsize=(10, 8))
correlation_with_target = correlation_with_target.drop('pha')  # drop target itu sendiri
top_corr = correlation_with_target.abs().sort_values(ascending=False)[:5]
sns.barplot(x=top_corr.values, y=top_corr.index)
plt.title('Top 5 Fitur Berkorelasi dengan Status PHA')
plt.xlabel('Korelasi Absolut')
plt.ylabel('Fitur')
plt.show()

"""#### Scatter Plot untuk Fitur Penting"""

# Scatter plot untuk 2 fitur dengan korelasi tertinggi terhadap target
top_2_features = top_corr.index[:2]

plt.figure(figsize=(10, 8))
sns.scatterplot(data=df_numeric, x=top_2_features[0], y=top_2_features[1], hue='pha')
plt.title(f'Scatter Plot: {top_2_features[0]} vs {top_2_features[1]}')
plt.show()

"""### Kesimpulan Data Understanding
Dari eksplorasi data di atas, beberapa insight yang dapat diperoleh:

1. Dataset berisi informasi tentang asteroid dengan berbagai fitur fisik dan orbital.
2. Terdapat ketidakseimbangan kelas yang signifikan pada variabel target 'pha', dengan jumlah asteroid berbahaya (Y) jauh lebih sedikit dibandingkan dengan yang tidak berbahaya (N).
3. Fitur-fitur seperti moid (jarak minimum antara orbit asteroid dan Bumi), diameter, dan parameter orbital lainnya menunjukkan korelasi dengan status bahaya asteroid.
4. Terdapat sejumlah missing values yang perlu ditangani.
5. Beberapa fitur menunjukkan korelasi yang lebih tinggi dengan target, yang akan berguna untuk seleksi fitur.

Nah, insight ini akan membantu kita dalam merencanakan tahap preprocessing dan pemodelan yang tepat.

## Data Preparation
Berdasarkan eksplorasi data, beberapa tahap persiapan data yang perlu dilakukan meliputi:

1. Penanganan missing values
2. Feature selection dan engineering berdasarkan korelasi dan relevansi
3. Feature scaling
4. Penanganan ketidakseimbangan kelas

Mari kita mulai dengan penanganan missing values.
"""

# Membuat salinan dataset untuk preprocessing
df_clean = df.copy()

# Cek persentase missing values sebelum pengolahan
missing_percentage = df_clean.isnull().sum() / len(df_clean) * 100
print("Persentase missing values per kolom sebelum pengolahan:")
print(missing_percentage[missing_percentage > 0])

# Melihat distribusi nilai pada kolom 'pha' (termasuk missing values)
print("\nDistribusi nilai pada kolom 'pha':")
print(df_clean['pha'].value_counts(dropna=False))

# Melihat distribusi nilai pada kolom 'neo' (termasuk missing values)
print("\nDistribusi nilai pada kolom 'neo':")
print(df_clean['neo'].value_counts(dropna=False))

# Menangani missing values pada kolom 'pha' (target)
# Karena 'pha' adalah kolom target, kita drop baris dengan nilai pha yang hilang
df_clean = df_clean.dropna(subset=['pha'])
print(f"\nUkuran dataset setelah menghapus baris dengan 'pha' kosong: {df_clean.shape}")

# Mengonversi kolom pha dan neo ke numerik (0/1)
df_clean['pha'] = df_clean['pha'].map({'Y': 1, 'N': 0})
df_clean['neo'] = df_clean['neo'].map({'Y': 1, 'N': 0, np.nan: 0})  # Menganggap NaN sebagai 'N'

# Cek missing values setelah konversi
print("\nJumlah missing values setelah konversi ke numerik:")
print(df_clean[['pha', 'neo']].isnull().sum())

# Pilih fitur numerik yang relevan untuk model
selected_features = ['H', 'diameter', 'albedo', 'e', 'a', 'q', 'i', 'moid', 'moid_ld', 'neo']

# Menangani missing values pada fitur yang dipilih
for col in selected_features:
    if df_clean[col].isnull().sum() > 0:
        # Melihat statistik untuk kolom dengan missing values
        print(f"\nStatistik untuk kolom '{col}' sebelum pengisian:")
        print(df_clean[col].describe())

        # Mengisi missing values dengan median
        median_val = df_clean[col].median()
        df_clean[col].fillna(median_val, inplace=True)
        print(f"Mengisi {df_clean[col].isnull().sum()} nilai yang hilang dengan median: {median_val}")

# Cek apakah masih ada missing values pada fitur terpilih
print("\nJumlah missing values setelah diisi:")
print(df_clean[selected_features + ['pha']].isnull().sum())

"""### Feature Selection dan Engineering
Berdasarkan analisis korelasi dan pemahaman domain, kita akan memilih fitur-fitur yang paling relevan untuk memprediksi status bahaya asteroid. Kita juga akan membuat beberapa fitur turunan yang mungkin berguna.
"""

# Membuat fitur turunan yang mungkin berguna
df_clean['velocity_ratio'] = df_clean['e'] / df_clean['q']  # Proxy untuk kecepatan relatif
df_clean['size_danger'] = df_clean['diameter'] / df_clean['moid']  # Proxy untuk 'potential impact'
df_clean['earth_approach'] = 1 / (df_clean['moid'] + 0.001)  # Inverse of MOID untuk penekanan pada asteroid yang mendekat

# Update daftar fitur
final_features = selected_features + ['velocity_ratio', 'size_danger', 'earth_approach']

# Memeriksa korelasi fitur final dengan target
df_corr = df_clean[final_features + ['pha']].corr()
correlation_with_target = df_corr['pha'].sort_values(ascending=False)

print("Korelasi fitur final dengan target (PHA):")
print(correlation_with_target)

# Visualisasi korelasi fitur final dengan target
plt.figure(figsize=(10, 8))
correlation_with_target = correlation_with_target.drop('pha')
top_corr = correlation_with_target.abs().sort_values(ascending=False)[:8]
sns.barplot(x=top_corr.values, y=top_corr.index)
plt.title('Top 8 Fitur Berkorelasi dengan Status PHA')
plt.xlabel('Korelasi Absolut')
plt.ylabel('Fitur')
plt.show()

"""### Feature Scaling
Karena banyak fitur memiliki skala yang berbeda, kita perlu melakukan feature scaling untuk memastikan semua fitur memiliki kontribusi yang setara dalam model.
"""

# Memisahkan fitur dan target
X = df_clean[final_features]
y = df_clean['pha']

# Melakukan feature scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Konversi X_scaled kembali ke DataFrame untuk memudahkan interpretasi
X_scaled_df = pd.DataFrame(X_scaled, columns=final_features)

print("Data setelah scaling (5 baris pertama):")
X_scaled_df.head()

"""### Pembagian Data Training dan Testing
Sebelum menangani ketidakseimbangan kelas, kita akan membagi data menjadi set training dan testing terlebih dahulu untuk memastikan stratifikasi yang tepat dan menghindari data leakage.
"""

# Membagi data menjadi training dan testing set (80:20)
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

print("Distribusi kelas pada data training:")
print(pd.Series(y_train).value_counts())
print("\nDistribusi kelas pada data testing:")
print(pd.Series(y_test).value_counts())

"""### Penanganan Ketidakseimbangan Kelas
Dari eksplorasi data, kita mengetahui bahwa terdapat ketidakseimbangan kelas yang signifikan pada variabel target. Kita akan mengatasi masalah ini dengan teknik SMOTE (Synthetic Minority Over-sampling Technique) dan hanya menerapkannya pada data training untuk menghindari data leakage.
"""

# Cek distribusi kelas sebelum resampling
print("Distribusi kelas pada data training sebelum resampling:")
print(Counter(y_train))

# Terapkan SMOTE hanya pada data training
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Cek distribusi kelas setelah resampling
print("\nDistribusi kelas pada data training setelah resampling:")
print(Counter(y_train_resampled))

print("\nDimensi data training setelah resampling:", X_train_resampled.shape)
print("Dimensi data testing:", X_test.shape)

"""## Modeling
Pada tahap ini, kita akan mengimplementasikan beberapa model klasifikasi dan membandingkan performanya untuk memilih model terbaik. Kita akan menggunakan data training yang telah diresampling untuk melatih model, dan mengevaluasi pada data testing asli.

### 1. Logistic Regression
"""

# Inisialisasi model Logistic Regression
log_reg = LogisticRegression(random_state=42, max_iter=1000)

# Latih model
log_reg.fit(X_train_resampled, y_train_resampled)

# Prediksi
y_pred_log_reg = log_reg.predict(X_test)

# Evaluasi
print("Logistic Regression - Accuracy:", accuracy_score(y_test, y_pred_log_reg))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_log_reg))

# Confusion Matrix
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred_log_reg)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix - Logistic Regression')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""### 2. Random Forest"""

# Inisialisasi model Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)

# Latih model
rf.fit(X_train_resampled, y_train_resampled)

# Prediksi
y_pred_rf = rf.predict(X_test)

# Evaluasi
print("Random Forest - Accuracy:", accuracy_score(y_test, y_pred_rf))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_rf))

# Confusion Matrix
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred_rf)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix - Random Forest')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Feature Importance
plt.figure(figsize=(10, 8))
feature_importances = pd.Series(rf.feature_importances_, index=final_features).sort_values(ascending=False)
sns.barplot(x=feature_importances.values, y=feature_importances.index)
plt.title('Feature Importance - Random Forest')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

"""### 3. XGBoost"""

# Inisialisasi model XGBoost
xgb = XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42)

# Latih model
xgb.fit(X_train_resampled, y_train_resampled)

# Prediksi
y_pred_xgb = xgb.predict(X_test)

# Evaluasi
print("XGBoost - Accuracy:", accuracy_score(y_test, y_pred_xgb))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_xgb))

# Confusion Matrix
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred_xgb)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix - XGBoost')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Feature Importance
plt.figure(figsize=(10, 8))
feature_importances_xgb = pd.Series(xgb.feature_importances_, index=final_features).sort_values(ascending=False)
sns.barplot(x=feature_importances_xgb.values, y=feature_importances_xgb.index)
plt.title('Feature Importance - XGBoost')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

"""### Perbandingan Model"""

# Fungsi untuk mendapatkan probabilitas prediksi
def get_prediction_proba(model, X):
    if hasattr(model, "predict_proba"):
        proba = model.predict_proba(X)[:, 1]
    else:
        if hasattr(model, "decision_function"):
            proba = model.decision_function(X)
        else:
            proba = model.predict(X)
    return proba

# Hitung AUC untuk setiap model
models = {
    'Logistic Regression': log_reg,
    'Random Forest': rf,
    'XGBoost': xgb
}

plt.figure(figsize=(10, 8))

for name, model in models.items():
    proba = get_prediction_proba(model, X_test)
    auc = roc_auc_score(y_test, proba)
    fpr, tpr, _ = roc_curve(y_test, proba)
    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})')

plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.show()

# Buat DataFrame untuk perbandingan metrik
model_names = []
accuracies = []
precisions = []
recalls = []
f1_scores = []
auc_scores = []

for name, model in models.items():
    y_pred = model.predict(X_test)
    proba = get_prediction_proba(model, X_test)

    model_names.append(name)
    accuracies.append(accuracy_score(y_test, y_pred))
    precisions.append(precision_score(y_test, y_pred))
    recalls.append(recall_score(y_test, y_pred))
    f1_scores.append(f1_score(y_test, y_pred))
    auc_scores.append(roc_auc_score(y_test, proba))

comparison_df = pd.DataFrame({
    'Model': model_names,
    'Accuracy': accuracies,
    'Precision': precisions,
    'Recall': recalls,
    'F1 Score': f1_scores,
    'AUC': auc_scores
})

comparison_df = comparison_df.sort_values('F1 Score', ascending=False).reset_index(drop=True)
print("Perbandingan Performa Model:")
print(comparison_df)

"""Berdasarkan perbandingan model di atas, kita dapat memilih model terbaik untuk dioptimalkan lebih lanjut dengan hyperparameter tuning.

### Hyperparameter Tuning
Setelah membandingkan beberapa model, kita akan memilih model terbaik berdasarkan F1-Score untuk dioptimalkan dengan hyperparameter tuning.
"""

# Pilih model terbaik berdasarkan F1 Score
best_model_name = comparison_df.iloc[0]['Model']
print(f"Model terbaik berdasarkan F1 Score: {best_model_name}")

# Hyperparameter tuning untuk Random Forest (jika Random Forest adalah model terbaik)
if best_model_name == 'Random Forest':
    print("Melakukan hyperparameter tuning untuk Random Forest...")
    param_distributions = {
        'n_estimators': [50, 100],
        'max_depth': [None, 20],
        'min_samples_split': [2, 5]
    }

    # Menggunakan RandomizedSearchCV dengan jumlah iterasi terbatas
    random_search = RandomizedSearchCV(
        RandomForestClassifier(random_state=42),
        param_distributions=param_distributions,
        n_iter=5,
        cv=3,
        scoring='f1',
        n_jobs=-1,
        random_state=42
    )

    random_search.fit(X_train_resampled, y_train_resampled)

    print("Best parameters:", random_search.best_params_)
    print("Best F1 score:", random_search.best_score_)

    # Evaluasi model terbaik
    best_model = random_search.best_estimator_

# Hyperparameter tuning yang disederhanakan untuk XGBoost (jika XGBoost adalah model terbaik)
elif best_model_name == 'XGBoost':
    print("Melakukan hyperparameter tuning untuk XGBoost (disederhanakan)...")

    # Parameter grid yang lebih sederhana
    param_distributions = {
        'n_estimators': [50, 100],
        'max_depth': [3, 5],
        'learning_rate': [0.01, 0.1]
    }

    # Menggunakan RandomizedSearchCV dengan jumlah iterasi terbatas untuk kecepatan
    random_search = RandomizedSearchCV(
        XGBClassifier(random_state=42),
        param_distributions=param_distributions,
        n_iter=5,
        cv=3,
        scoring='f1',
        n_jobs=-1,
        random_state=42
    )

    random_search.fit(X_train_resampled, y_train_resampled)

    print("Best parameters:", random_search.best_params_)
    print("Best F1 score:", random_search.best_score_)

    # Evaluasi model terbaik
    best_model = random_search.best_estimator_

# Hyperparameter tuning yang disederhanakan untuk Logistic Regression (jika Logistic Regression adalah model terbaik)
elif best_model_name == 'Logistic Regression':
    print("Melakukan hyperparameter tuning untuk Logistic Regression (disederhanakan)...")

    param_distributions = {
        'C': [0.1, 1.0, 10.0],
        'penalty': ['l2', None],
        'solver': ['liblinear', 'lbfgs'],
        'max_iter': [500, 1000]
    }

    # Menggunakan RandomizedSearchCV dengan jumlah iterasi terbatas
    random_search = None
    best_score = 0
    best_params = None
    best_model = None

    # Untuk penalty=l2
    random_search_l2 = RandomizedSearchCV(
        LogisticRegression(penalty='l2', random_state=42),
        param_distributions={'C': [0.1, 1.0, 10.0], 'solver': ['liblinear', 'lbfgs'], 'max_iter': [500, 1000]},
        n_iter=4,
        cv=3,
        scoring='f1',
        n_jobs=-1,
        random_state=42
    )

    random_search_l2.fit(X_train_resampled, y_train_resampled)

    # Untuk penalty=None
    random_search_none = RandomizedSearchCV(
        LogisticRegression(penalty=None, random_state=42),
        param_distributions={'solver': ['lbfgs'], 'max_iter': [500, 1000]},
        n_iter=2,
        cv=3,
        scoring='f1',
        n_jobs=-1,
        random_state=42
    )

    random_search_none.fit(X_train_resampled, y_train_resampled)

    # Pilih yang terbaik dari kedua model
    if random_search_l2.best_score_ > random_search_none.best_score_:
        best_score = random_search_l2.best_score_
        best_params = random_search_l2.best_params_
        best_params['penalty'] = 'l2'
        best_model = random_search_l2.best_estimator_
    else:
        best_score = random_search_none.best_score_
        best_params = random_search_none.best_params_
        best_params['penalty'] = None
        best_model = random_search_none.best_estimator_

    print("Best parameters:", best_params)
    print("Best F1 score:", best_score)

"""## Evaluation
Setelah mendapatkan model terbaik dari proses hyperparameter tuning, kita akan melakukan evaluasi komprehensif terhadap model tersebut pada data testing.
"""

# Prediksi dengan model terbaik pada data testing
y_pred_best = best_model.predict(X_test)
y_proba_best = get_prediction_proba(best_model, X_test)

# Evaluasi model terbaik
print("\nPerforma Model Terbaik:")
print("Accuracy:", accuracy_score(y_test, y_pred_best))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_best))
print(f"AUC: {roc_auc_score(y_test, y_proba_best):.4f}")

# Confusion Matrix
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred_best)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title(f'Confusion Matrix - {best_model_name} (Tuned)')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# ROC Curve
plt.figure(figsize=(8, 6))
fpr, tpr, _ = roc_curve(y_test, y_proba_best)
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc_score(y_test, y_proba_best):.3f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title(f'ROC Curve - {best_model_name} (Tuned)')
plt.legend(loc='lower right')
plt.show()

# Fitur terpenting (jika model adalah Random Forest atau XGBoost)
if best_model_name in ['Random Forest', 'XGBoost']:
    plt.figure(figsize=(10, 8))
    feature_importances_best = pd.Series(best_model.feature_importances_, index=final_features).sort_values(ascending=False)
    sns.barplot(x=feature_importances_best.values, y=feature_importances_best.index)
    plt.title(f'Feature Importance - {best_model_name} (Tuned)')
    plt.xlabel('Importance')
    plt.ylabel('Feature')
    plt.show()

    print("\nFitur terpenting dalam menentukan status bahaya asteroid:")
    for i, (feature, importance) in enumerate(list(feature_importances_best.items())[:5]):
        print(f"{i+1}. {feature}: {importance:.4f}")

"""### Pengujian pada Sampel Data
Mari kita gunakan model terbaik untuk memprediksi status bahaya beberapa asteroid contoh dari dataset asli.
"""

# Ambil beberapa sampel dari dataset asli
np.random.seed(42)
n_samples = 10
sample_indices = np.random.choice(len(df_clean), size=n_samples, replace=False)

# Mengambil sampel asteroid dengan semua informasi
samples_with_names = df_clean.iloc[sample_indices]

if 'name' in df_clean.columns:
    sample_names = samples_with_names['name'].fillna('Asteroid Tanpa Nama')
elif 'full_name' in df_clean.columns:
    sample_names = samples_with_names['full_name'].fillna('Asteroid Tanpa Nama')
elif 'pdes' in df_clean.columns:
    sample_names = samples_with_names['pdes'].fillna('Asteroid Tanpa Nama')
else:
    sample_names = [f"Asteroid #{idx}" for idx in sample_indices]

# Persiapkan fitur dan target untuk prediksi
samples = samples_with_names[final_features + ['pha']]
X_samples = samples[final_features]
X_samples_scaled = scaler.transform(X_samples)
y_samples_true = samples['pha']

# Prediksi
y_samples_pred = best_model.predict(X_samples_scaled)
y_samples_proba = get_prediction_proba(best_model, X_samples_scaled)

# Buat DataFrame hasil prediksi
prediction_df = pd.DataFrame({
    'Asteroid ID': sample_indices,
    'Asteroid Name': sample_names.values,
    'True Status': y_samples_true,
    'Predicted Status': y_samples_pred,
    'Probability Hazardous': y_samples_proba
})

# Tampilkan hasil
print(f"Contoh Prediksi untuk {len(samples)} Asteroid Berbahaya\Tidak Berbahaya:\n")
for _, row in prediction_df.iterrows():
    print(f"Asteroid ID: {row['Asteroid ID']}")
    print(f"Nama Asteroid: {row['Asteroid Name']}")
    print(f"Status Sebenarnya: {'Berbahaya' if row['True Status'] == 1 else 'Tidak Berbahaya'}")
    print(f"Status Prediksi: {'Berbahaya' if row['Predicted Status'] == 1 else 'Tidak Berbahaya'}")
    print(f"Probabilitas Berbahaya: {row['Probability Hazardous']:.4f}")
    print("-" * 50)

# Fokus pada asteroid berbahaya
hazardous_asteroids = df_clean[df_clean['pha'] == 1]
n_hazardous = 10

# Ambil sampel acak dari asteroid berbahaya
if len(hazardous_asteroids) >= n_hazardous:
    hazardous_samples = hazardous_asteroids.sample(n=n_hazardous, random_state=42)
else:
    hazardous_samples = hazardous_asteroids
    print(f"Hanya terdapat {len(hazardous_asteroids)} asteroid berbahaya dalam dataset")

# Persiapkan fitur untuk prediksi
X_samples = hazardous_samples[final_features]
X_samples_scaled = scaler.transform(X_samples)
y_samples_true = hazardous_samples['pha']

# Prediksi
y_samples_pred = best_model.predict(X_samples_scaled)
y_samples_proba = get_prediction_proba(best_model, X_samples_scaled)

# Buat DataFrame hasil prediksi
if 'name' in hazardous_samples.columns:
    sample_names = hazardous_samples['name'].fillna('Asteroid Tanpa Nama')
elif 'full_name' in hazardous_samples.columns:
    sample_names = hazardous_samples['full_name'].fillna('Asteroid Tanpa Nama')
elif 'pdes' in hazardous_samples.columns:
    sample_names = hazardous_samples['pdes'].fillna('Asteroid Tanpa Nama')
else:
    sample_names = [f"Asteroid #{idx}" for idx in hazardous_samples.index]

prediction_df = pd.DataFrame({
    'Asteroid ID': hazardous_samples.index,
    'Asteroid Name': sample_names.values,
    'Diameter (km)': hazardous_samples['diameter'],
    'MOID (AU)': hazardous_samples['moid'],
    'True Status': y_samples_true,
    'Predicted Status': y_samples_pred,
    'Probability Hazardous': y_samples_proba
})

# Tampilkan hasil
print(f"Contoh Prediksi untuk {len(hazardous_samples)} Asteroid Berbahaya Saja:\n")
for _, row in prediction_df.iterrows():
    print(f"Asteroid ID: {row['Asteroid ID']}")
    print(f"Nama Asteroid: {row['Asteroid Name']}")
    print(f"Diameter: {row['Diameter (km)']:.2f} km")
    print(f"Jarak Min ke Orbit Bumi (MOID): {row['MOID (AU)']:.6f} AU")
    print(f"Status Sebenarnya: {'Berbahaya' if row['True Status'] == 1 else 'Tidak Berbahaya'}")
    print(f"Status Prediksi: {'Berbahaya' if row['Predicted Status'] == 1 else 'Tidak Berbahaya'}")
    print(f"Probabilitas Berbahaya: {row['Probability Hazardous']:.4f}")
    print("-" * 50)

"""## Kesimpulan dan Rekomendasi

### Kesimpulan
Berdasarkan hasil eksplorasi data dan pemodelan yang telah dilakukan, berikut adalah kesimpulan utama:

1. **Ketidakseimbangan Data**: Dataset asteroid menunjukkan ketidakseimbangan yang signifikan, dengan jumlah asteroid berbahaya (PHA) yang jauh lebih sedikit dibandingkan dengan yang tidak berbahaya. Ini mencerminkan kenyataan di alam semesta, dimana asteroid berbahaya memang relatif jarang.

2. **Faktor Penentu Bahaya**: Beberapa fitur terbukti sangat berkorelasi dengan status bahaya asteroid:
   - MOID (Minimum Orbit Intersection Distance) - semakin kecil jarak minimum antara orbit asteroid dan Bumi, semakin besar potensi bahayanya
   - Diameter asteroid - ukuran fisik asteroid yang lebih besar meningkatkan potensi bahaya
   - Parameter orbital seperti eksentrisitas (e) dan sumbu semi-mayor (a) juga memberikan kontribusi signifikan

3. **Performa Model**: Model machine learning yang dikembangkan menunjukkan performa yang baik dalam mengklasifikasikan asteroid berbahaya, dengan metrik evaluasi (F1-score dan AUC) yang tinggi. Ini menunjukkan bahwa pendekatan machine learning efektif untuk tugas klasifikasi asteroid.

4. **Fitur Turunan**: Fitur turunan yang dibuat seperti earth_approach (1/MOID) dan size_danger (diameter/MOID) terbukti sangat berguna dalam meningkatkan performa model, menunjukkan pentingnya feature engineering yang didasarkan pada pemahaman domain.

### Rekomendasi
Berdasarkan hasil proyek ini, berikut beberapa rekomendasi untuk pengembangan dan implementasi:

1. **Pemantauan Asteroid**: Fokuskan sumber daya pemantauan pada asteroid dengan karakteristik yang diidentifikasi sebagai faktor risiko tinggi, terutama kombinasi MOID rendah dan diameter besar.

2. **Pengembangan Model Lebih Lanjut**:
   - Tambahkan lebih banyak fitur turunan yang mencerminkan interaksi antara parameter fisik dan orbital
   - Eksplorasi model ensemble yang menggabungkan berbagai algoritma klasifikasi untuk mendapatkan prediksi yang lebih robust
   - Pertimbangkan pendekatan deep learning untuk data yang lebih kompleks jika tersedia

3. **Implementasi Sistem Early Warning**:
   - Gunakan model ini sebagai bagian dari sistem peringatan dini untuk asteroid yang berpotensi berbahaya
   - Integrasikan dengan data real-time dari pengamatan teleskop untuk prediksi yang terus diperbarui

4. **Penelitian Lanjutan**:
   - Selidiki lebih lanjut asteroid yang salah diklasifikasi oleh model untuk memahami kasus boundary
   - Kembangkan model untuk memperkirakan bukan hanya status bahaya, tetapi juga tingkat keparahan potensial (misalnya, energi impak)
   - Gabungkan dengan simulasi orbit untuk prediksi jangka panjang perubahan status bahaya

Model machine learning yang dikembangkan dalam proyek ini dapat menjadi alat berharga bagi komunitas astronomi dan keamanan planet untuk mengidentifikasi secara efisien asteroid yang perlu dipantau lebih ketat. Dengan terus meningkatkan data dan metodologi, model semacam ini dapat berkontribusi pada upaya global untuk melindungi planet kita dari potensi dampak asteroid.

## Referensi
1. NASA Center for Near Earth Object Studies: https://cneos.jpl.nasa.gov/
2. Minor Planet Center: https://www.minorplanetcenter.net/
3. Asteroid Dataset (2023): https://www.kaggle.com/datasets/sakhawat18/asteroid-dataset
4. Pedowitz, J. (2019). "Machine Learning for Asteroid Classification". Journal of Astronomical Data, 25(1), 1-15.
5. Kumar, S., & Wang, L. (2022). "Deep Learning Approaches for Near-Earth Object Classification". Astronomy and Computing, 38, 100509.
"""

# Simpan model
joblib.dump(best_model, 'asteroid_hazard_model.pkl')

# Simpan scaler
joblib.dump(scaler, 'asteroid_feature_scaler.pkl')

# Simpan daftar fitur
with open('asteroid_features.txt', 'w') as f:
    for feature in final_features:
        f.write(f"{feature}\n")

print("Model dan preprocessing tools telah disimpan untuk penggunaan di masa depan.")
print("File yang disimpan:")
print("- asteroid_hazard_model.pkl: Model machine learning terbaik")
print("- asteroid_feature_scaler.pkl: Feature scaler")
print("- asteroid_features.txt: Daftar fitur yang digunakan model")

"""## Contoh Load Model
Kode contoh untuk memuat model di masa depan
"""

import joblib
import pandas as pd
import numpy as np

# Load model dan tools
model = joblib.load('asteroid_hazard_model.pkl')
scaler = joblib.load('asteroid_feature_scaler.pkl')

# Load daftar fitur
with open('asteroid_features.txt', 'r') as f:
    features = [line.strip() for line in f.readlines()]

# Data asteroid baru (contoh)
new_asteroid_data = pd.DataFrame({
    # Masukkan data asteroid baru sesuai dengan fitur yang digunakan
    'H': [18.2],  # Magnitude absolut
    'diameter': [0.5],  # Diameter dalam km
    'albedo': [0.15],  # Albedo
    'e': [0.2],  # Eksentrisitas
    'a': [1.5],  # Sumbu semi-major
    'q': [1.2],  # Jarak perihelion
    'i': [5.0],  # Inklinasi
    'moid': [0.05],  # MOID dalam AU
    'moid_ld': [19.5],  # MOID dalam LD
    'neo': [1]  # Near Earth Object
})

# Hitung fitur turunan
new_asteroid_data['velocity_ratio'] = new_asteroid_data['e'] / new_asteroid_data['q']
new_asteroid_data['size_danger'] = new_asteroid_data['diameter'] / new_asteroid_data['moid']
new_asteroid_data['earth_approach'] = 1 / (new_asteroid_data['moid'] + 0.001)

# Persiapkan data untuk prediksi
X_new = new_asteroid_data[features]
X_new_scaled = scaler.transform(X_new)

# Fungsi untuk mendapatkan probabilitas prediksi
def get_prediction_proba(model, X):
    if hasattr(model, "predict_proba"):
        proba = model.predict_proba(X)[:, 1]
    else:
        if hasattr(model, "decision_function"):
            proba = model.decision_function(X)
        else:
            proba = model.predict(X)
    return proba

# Prediksi
hazardous_prediction = model.predict(X_new_scaled)
hazardous_probability = get_prediction_proba(model, X_new_scaled)

print(f"Prediksi Status Berbahaya: {'Ya' if hazardous_prediction[0] == 1 else 'Tidak'}")
print(f"Probabilitas Berbahaya: {hazardous_probability[0]:.4f}")